# %% [markdown]
# # üéØ BERT Tone Classification (RuBERT)
# –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: –ø–æ–∑–∏—Ç–∏–≤ / –Ω–µ–≥–∞—Ç–∏–≤ / –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π.
# –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å `DeepPavlov/rubert-base-cased` –∏ –¥–æ–æ–±—É—á–∞–µ–º –µ—ë –Ω–∞ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.

# %% üì¶ –ò–º–ø–æ—Ä—Ç—ã –∏ —Å–∏—Å—Ç–µ–º–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.utils import to_categorical
import tensorflow as tf

from transformers import AutoTokenizer, TFAutoModel

# üîá –û—Ç–∫–ª—é—á–∞–µ–º –ª–∏—à–Ω–∏–µ –ª–æ–≥–∏ –∏ –≤–∫–ª—é—á–∞–µ–º GPU (–µ—Å–ª–∏ –µ—Å—Ç—å)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print("‚úÖ GPU –¥–æ—Å—Ç—É–ø–µ–Ω.")
    except RuntimeError as e:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å GPU:", e)
else:
    print("‚ùå GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU.")
#%%
import tensorflow as tf
print("‚úÖ TF version:", tf.__version__)
print("üì¶ GPU devices:", tf.config.list_physical_devices("GPU"))
#%%

df = pd.read_csv(r"C:\Users\StasAndLiza\port\nlp\data\raw\dataset.csv")

# –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
df = df.rename(columns={
    "comment_txt": "text",
    "sentiment_txt": "tone",
    "parent_class_txt": "level_1",
    "child_class_txt": "level_2"
})

# –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∫ —á–∏—Å—Ç–æ–º—É –≤–∏–¥—É
df["tone"] = df["tone"].str.replace("SENTIMENT_", "", regex=False).str.capitalize()
df = df[df["tone"] != "UNSPECIFIED"]

# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
texts = df["text"].tolist()
lbl = LabelEncoder()
y = lbl.fit_transform(df["tone"])
y_cat = to_categorical(y)

# %% üî† –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RuBERT
MODEL_NAME = "DeepPavlov/rubert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

encodings = tokenizer(
    texts,
    padding="max_length",
    truncation=True,
    max_length=128,
    return_tensors="np"
)

X_ids = np.array(encodings["input_ids"])
X_mask = np.array(encodings["attention_mask"])

# %% üß™ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
X_train_ids, X_test_ids, y_train, y_test = train_test_split(
    X_ids, y_cat, test_size=0.2, stratify=y_cat.argmax(axis=1), random_state=42
)

X_train_mask, X_test_mask = train_test_split(
    X_mask, test_size=0.2, stratify=y_cat.argmax(axis=1), random_state=42
)

# %% üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –±–∞–∑–µ BERT
bert_model = TFAutoModel.from_pretrained(MODEL_NAME, from_pt=True)

input_ids = Input(shape=(128,), dtype=tf.int32, name="input_ids")
attention_mask = Input(shape=(128,), dtype=tf.int32, name="attention_mask")

# –ò–∑–≤–ª–µ–∫–∞–µ–º [CLS] —Ç–æ–∫–µ–Ω –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
bert_outputs = bert_model(input_ids, attention_mask=attention_mask)
cls_output = bert_outputs.last_hidden_state[:, 0, :]  # [CLS] —Ç–æ–∫–µ–Ω

x = Dropout(0.3)(cls_output)
x = Dense(64, activation="relu")(x)
output = Dense(3, activation="softmax")(x)

model = Model(inputs=[input_ids, attention_mask], outputs=output)
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

model.summary()

# %% üöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
history = model.fit(
    [X_train_ids, X_train_mask],
    y_train,
    validation_data=([X_test_ids, X_test_mask], y_test),
    epochs=3,
    batch_size=16
)

# %% üìà –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
plt.plot(history.history["accuracy"], label="Train")
plt.plot(history.history["val_accuracy"], label="Validation")
plt.title("BERT Tone Classification Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# %% üìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
y_pred = model.predict([X_test_ids, X_test_mask]).argmax(axis=1)
y_true = y_test.argmax(axis=1)

print(classification_report(y_true, y_pred, target_names=lbl.classes_))

conf = confusion_matrix(y_true, y_pred)
sns.heatmap(conf, annot=True, fmt="d", cmap="Blues",
            xticklabels=lbl.classes_, yticklabels=lbl.classes_)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.tight_layout()
plt.show()

# %%
